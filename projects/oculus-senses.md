---
title: Adapting VR to Study the Senses in Context
layout: no-sidebar-page
---

Most psychophysics research relies on simple computer interfaces consisting of flashes of white light and bursts of white noise to get at how the human sensory system works. Recently, however, this body of research has been criticized for telling us little about how the brain make sense of information coming from separate channels (i.e., vision, hearing, touch) in the noisier and busier contexts of the real world.

<iframe src="https://giphy.com/embed/3ohc136R0ywN8zb2EM" width="720" height="450" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>
>Representation of a classic minimal psychophysics experiment. The research participant presses a button when they see the white circle

<br>

Working in Prof. Leslie Kwakye's behavioral neuroscience lab at Oberlin College, I developed a “virtual reality” (VR) application that could be used to study how the human brain integrates audio and visual information in more complex environments like the ones we experience in real life. Our project used the Oculus Rift HMD, a surround sound speaker system, Python and 3DS Max to create an immersive version of classic multisensory experiments. For this project, research and interface design were closely linked through every stage of the process.

<br>
![...](/assets/vr-room.jpg){:width="100%"}
>Sound-proofed VR room with Oculus Rift and mounted surround-sound speakers which we used to study audio-visual integration in the brain

<br>

I designed the VR environments by first making simple wireframes in collaboration with my professor and team member. These mock-ups emphasized different kinds of simple or complex environment features we could exploit in the study. My teammate and I each built prototype environments we could use in the experiment. Several versions of the prototype, which included a number of separate prototype environments, were tested on students and volunteers.

![...](/assets/task-sketches.jpg){:width="100%"}
>Sketches of trial structure schematic and room dimension testing

<br>

![...](/assets/task-flow.jpg){:width="100%"}
>Mock-up of experiment run-through

<br>

I was responsible for conducting final-stage user testing for the comfort and ease of the task. With data collection features I had built into the prototype app, I was also able to use interaction and response data to narrow-down our environments which represented a range of simple and more information-dense contexts. I then created the final versions of the virtual environment and task app based on these data.

<br>
![...](/assets/vr-exp.png){:width="100%"}
>One of the virtual environments used in the study

<br>

Two finalized versions of the app were used to conduct a behavioral neuroscience study. Though we ultimately ended up discarding the results from one of the experiments, the other led to interesting and unexpected findings about how the brain integrates sensory information in complex or chaotic environments. I presented this research at [national conferences](http://www.abstractsonline.com/Plan/ViewAbstract.aspx?mID=3744&sKey=8ffdb9bb-e46a-4d5d-8eba-d2ab4dd08884&cKey=b23bba56-576a-48aa-a886-c95fb61bb543&mKey=d0ff4555-8574-4fbb-b9d4-04eec8ba0c84), gave a related discussion-panel [talk](https://www.dropbox.com/s/wio5f70xiyqvpx7/synesthesia-symposium-april-15-updated-figures.pdf?dl=0) at an interdisciplinary symposium on synesthesia, and have submitted a manuscript to an academic journal for publication.


[Back to projects](../)
