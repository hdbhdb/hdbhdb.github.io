---
title: Adapting VR to Study the Senses in Context
layout: no-sidebar-page
---

Most psychophysics research has relied on simple computer interfaces consisting of flashes of white light and bursts of white noise to get at how the human sensory system works. Recently, however, this body of research has been complicated by mounting evidence that these simple studies tell us little about how the brain make sense of information coming from separate channels (i.e., vision, hearing, touch) in the noisier and more distracting contexts of the real world. So how is it possible to reconcile the knowledge about what the brain does out of context in a laboratory setting with the difficulty of studying what the brain does when it actually matters?

<iframe src="https://giphy.com/embed/3ohc136R0ywN8zb2EM" width="720" height="450" frameBorder="0" class="giphy-embed" allowFullScreen></iframe>
>Representation of a classic minimal psychophysics experiment. The research participant presses a button when they see the white circle or a noise that might accompany it.

<br>

As "virtual reality" technology continues to improve, it allows neuroscience research to fabricate complex environments and scenarios in order to study what the brain does in a setting that is both controlled and realistic. Working in Prof. Leslie Kwakye's behavioral neuroscience lab at Oberlin College, I developed a VR application that could be used to study how the human brain integrates audio and visual sensory information in more complex environments like the ones we experience in real life. Our project used the Oculus Rift HMD, a surround sound speaker system, and VR environments I built using Vizard, Python and 3DS Max to create an immersive version of classic multisensory psychophysics experiments.

<br>
![...](/assets/vr-room.jpg){:width="100%"}
>Sound-proofed VR room with Oculus Rift and mounted surround-sound speakers which we used to study audio-visual integration in the brain

<br>

I designed the VR environments by first making simple wireframes in collaboration with my professor and team members. These mock-ups emphasized different kinds of simple or complex environment features we could exploit in the study in order to get a range of complex and simple virtual worlds. By making some of the environments minimal and more like the classic psychophysics experiments, we could identify patterns in how the brain extracts multisensory information from environments that are more complex versus less complex. We first built prototype environments we could use in the experiment. Several versions of the prototype VR environments were tested on students and volunteers.

![...](/assets/task-sketches.jpg){:width="100%"}
>Sketches of trial structure schematic and room dimension testing

<br>

![...](/assets/task-flow.jpg){:width="100%"}
>Mock-up of experiment run-through

<br>

I was responsible for conducting final-stage user testing for the comfort and ease of the VR task. With data collection features I had built into the prototype app, I was also able to use motion and response data to narrow-down our environments to ones which represented an ideal range of simple and more information-dense contexts. I then created the final versions of the virtual environment and study app based on these data.

<br>
![...](/assets/vr-exp.png){:width="100%"}
>One of the virtual environments used in the study was made to look like the experiment room.

<br>

Two finalized versions of the app were used to conduct a two-part "multisensory detection" neuroscience study. One of these led to interesting and unexpected findings about how the brain integrates sensory information in more complex environments. Our research also served as a proof of concept for using VR to replicate and verify findings in sensory neuroscience. I presented this research at [national conferences](http://www.abstractsonline.com/Plan/ViewAbstract.aspx?mID=3744&sKey=8ffdb9bb-e46a-4d5d-8eba-d2ab4dd08884&cKey=b23bba56-576a-48aa-a886-c95fb61bb543&mKey=d0ff4555-8574-4fbb-b9d4-04eec8ba0c84) and gave a related discussion-panel [talk](https://www.dropbox.com/s/wio5f70xiyqvpx7/synesthesia-symposium-april-15-updated-figures.pdf?dl=0) at an interdisciplinary symposium on synesthesia. We have also submitted a manuscript to an academic journal for publication.

My work in the lab also included research planning and VR environment design for a follow-up VR study on "cross-modal attentional cuing" in complex environments which was completed after I had graduated.

![...](/assets/vr-exp-2.png){:width="100%"}
>Environment I designed for our second round of VR sensory studies.

[Back to projects](../)
